---
layout:     post
title:      "A crude intro to semiparametric statistics: part 1"
mathjax:    true
author:     Ruofan Wu
summary:    Something rigorous about what the heck are models, parameters, and maximum likelihood principle
---

## Elementary facts

First let's review some facts that is familiar to everyone: suppose our data is considered to be a bunch of i.i.d observations sampled from a probability distribution, i.e. $$ X_1, \ldots, X_n \sim \mathbb{P}_X $$, and we're interested in some **parameter** $$ \theta $$ for example *sample mean* $$ \theta = \int X d\mathbb{P}_X $$. Our first attempt toward this parameter is *point estimation*, where we form something called **estimator** $$ T_n $$ that is a function of our data, i.e. $$ T_n = T_n(X_1, \ldots, X_n) $$ that aims at estimating $$ \theta $$. Then a meaningful estimator, at least during our mathematical statistics courses, should have a basic property called **asymptotic normality**  
$$
\begin{align}\label{normal}
    \sqrt{n}\left( T_n - \theta \right) \rightsquigarrow N(0, \sigma(\theta))
\end{align}
$$  
Another important fact is that we know among those asymptotically normal estimators, the one with the "least" asymptotic variance is the **maximum likelihood estimator**. This roots from a key fact called Cramér–Rao bound saying for all estimators satisfying \eqref{normal}, their asymptotic variances are lower bounded by the inverse Fisher information of $$ \theta $$, i.e. under the simplest setup of one dimensional parameter:  
$$
\begin{align}\label{crbound}
    \sigma(\theta) \ge I^{-1}(\theta),\qquad I(\theta) = \mathbb{E}_{X} \left( \dfrac{\partial}{\partial \theta} l(\theta; X)\right)^2 
\end{align}
$$  
Here we inevitably touched the notion of **likelihood** with $$ l(\theta) $$ denoting the log-likelihood of $$ \theta $$, if we attempt to use likelihood, we're convincing ourselves that we've picked the right **model**, for now I don't want to make too many formal explanations so let's say $$ \mathbb{P}_X $$ comes from a family of distributions that is fully described through our parameter of interest, for example a Gaussian distribution with known variance and our goal is to estimate the mean. Under such beliefs, we write $$ \mathbb{P}_X $ as $ \mathbb{P}_{\theta} $$. If we make some abuse of notation by writing both the *probability mass function* and the *probability density function* as $$ p(x; \theta) $$ and $$ l(\theta; x) = \log p(x; \theta) $$, then the procedure of maximum likelihood:  
$$
\begin{align}
    \hat{\theta}_{\text{MLE}} \in \underset{\theta}{\arg\max} \sum_i l(\theta; X_i)
\end{align}
$$  
will attain the Cramér–Rao lower bound \eqref{crbound}. This is the classic **likelihood principle** which essentially states:  
><span class="smallcaps"> Forget about misspecification, our assumption is correct</span>  
<span class="smallcaps"> Write out the likelihood, and the story's over</span>  

Inferential procedures thereafter is also optimal in the sense that as long as you have your asymptotic variance minimal, the confidence interval constructed for the parameter is the shortest, and for a hypothesis testing problem, the ad-hoc adaptation of *likelihood ratio test* is the *uniformly most powerful* one.  
Okay upon investigating the above discussion, we've found really a lot of claims that "something should be nice for this paradigm to work", and I believe that almost everyone, statistician or not, will find the likelihood principle *overly optimistic*, yet it's not trivial however, to explain carefully the theoretical boundary of the likelihood paradigm. And the theory of semiparametrics should be established from a careful characterization of this paradigm. We throw several questions here:

What is a model, anyway?
What kind of parameter is nice?
Is the bound \eqref{crbound} really that strong?

## Model

Let's form our story in a very "big" setup: as long as $$ X_i $$s are i.i.d. random, their distribution $$ \mathbb{P}_X $$ should come from *the collection of all the probability distributions on the sample sapce* say $$ \mathcal{M} $$, here sample space means a Borel algebra on all possible values of $$ X $$, say $$ \left( \mathscr{X}, \mathscr{B} \right) $$, to further prevent unnecessary abstraction, we restrict this collection to be dominated by some $$ \sigma- $$ finite measure on $$ \nu $$ on $$ \left( \mathscr{X}, \mathscr{B} \right) $$ for example Lebesgue measure for continuous distributions and counting measure for discrete ones, and write this collection as $$ \mathcal{M}_{\nu} = \left\lbrace \mathbb{P} \in \mathcal{M}, \mathbb{P} \ll \nu \right\rbrace  $$, than a **model** $ \mathscr{P} $ should not go beyond this huge collection $ \mathcal{M}_{\nu} $, i.e. $ \mathscr{P} \subset \mathcal{M}_{\nu} $. Than it seems...  

> Holy shit, $ \mathcal{M}_{\nu} $ is too large...  

## Parameterization

The role of parameters is essentially to avoid always facing an awkward space of probability measures, and "linking" the model with some space which we could carry out procedures that works in practice. The easiest case is for sure *Euclidean space*, on which optimization procedures are useful for most of the problems. A parameterization is thus a way of linking $$ \mathscr{P} $$ and a subset $$ \Theta $$ of some finite dimensional Euclidean space $$ \mathbb{R}^d $$ where $$ d $$ stands for the dimension. Specifically, we view parameterization as a map $$ \theta \mapsto \mathbb{P}_{\theta} \in  \mathscr{P} $$.  

### Identifiability

The most important requirement of a nice parameterization is perhaps identifiability, saying that the map $$ \theta \mapsto \mathbb{P}_{\theta} $$ is *bijective*, i.e. different $$ \theta $$s shall maps to different measures. This is trivial if the data is univariate and we pick a univariate distribution family as the parameterization, like we assume a Gaussian model, but frequently violated in multivariate settings like regression and classification tasks, if we use a parameter set $$ \left\lbrace \beta_k\right\rbrace_{1 \le k \le 5}  $$ for a $$ 5 $$ class logistic regression, the multinomial probability model is invariant under equal-scale location shifts, i.e. 
$$
\begin{align*}
    \dfrac{\exp(\langle \beta_i + \epsilon, X \rangle)}{\sum_{j = 1}^{5}\exp(\langle \beta_j + \epsilon, X \rangle)} \equiv \dfrac{\exp(\langle \beta_i, X \rangle)}{\sum_{j = 1}^{5}\exp(\langle \beta_j, X \rangle)}, \qquad \forall 1\le i \le 5, \epsilon \in \mathbb{R}
\end{align*}
$$  
Identifiability is *crucial* for MLE procedures, this will be illustrated in later posts. For now, it seems okay to write $$ \mathscr{P} $$ as  
$$
\begin{align}
    \mathscr{P} = \left\lbrace \mathbb{P}_{\theta}: \theta \in \Theta \right\rbrace 
\end{align}
$$  

### Regularity of parameterizations

Now let's characterize what it means to be a *good parameterization*, indeed this amounts to inspect the properties of the map $$ \theta \mapsto \mathbb{P}_{\theta} $$, since the parameterization is considered bijective, it's straightforward to inspect the density, or Radon-Nikodym derivative with respect to $$ \nu $$, i.e. 
$$
\begin{align*}
    p_{\theta} := p(x; \theta) = \dfrac{d \mathbb{P}_{\theta}}{d \nu} (x)
\end{align*}
$$  
Then the map $$ \theta \mapsto p_{\theta} $$ could be interpreted as a map from $$ \Theta $ to $ L_1(\nu) $$. It won't cause much trouble if we proceed this way, however it turns out that a better characterization of this map in the context of asymptotics, is to make a little hack $$ s_{\theta} = \sqrt{p_{\theta}} $$. On one hand this makes the map $$ \theta \mapsto s_{\theta} $$ from $ \Theta $ to $ L_2(\nu) $, on the other hand, it coincides with *Hellinger metric* between two distributions  
$$
\begin{align*}
    d_H(P, Q) = \sqrt{\int (\sqrt{dP} - \sqrt{dQ})^2}
\end{align*}
$$  
Hence it would be quite reasonable if we make this map "smooth" w.r.t Hellinger metric, technically this requires the *Fréchet differentiability* with respect to norm $$ \left\| f \right\|_{\nu} = \int f^2 d\nu  $$, i.e. for every $$ \theta \in \Theta $$, there exists a vector $$ \dot{s}_{\theta} = \left( \dot{s}^1_{\theta}, \ldots, \dot{s}^d_{\theta}\right)  $$ with each component an element of $$ L_2(\nu) $$ such that:  
$$
\begin{align*}
    \left\| s_{\theta + h} - s_{\theta} - \langle \dot{s}_{\theta}, h \rangle \right\|_{\nu} = o(\left\| h\right\|_2 )
\end{align*}
$$  
where $$ h \rightarrow 0 $$ in $$ \mathbb{R}^d $. Moreover if the map $$ \theta \mapsto \dot{s}_{\theta} $$ is continuous in $$ L_2(\nu) $$ and the matrix $$ \int \dot{s}_{\theta} \dot{s}^\top_{\theta} d\nu $$ is nonsingular everywhere in $$ \Theta $$, we say that the parameterization $$ \theta \mapsto \mathbb{P}_{\theta} $$ is **regular**  
This property is referred to by van der Vaart [7, Chapter 5, page 65] as *differentiability in the quadratic mean*. The use of Fréchet derivative seems puzzling, but since the domain of the map is euclidean, it's not hard to verify that this is just a rephrasing of the original score function:  
$$
\begin{align*}
    &\dot{l}_{\theta} := \dfrac{\partial}{\partial \theta} l(\theta; X) 1_{\left( p_{\theta} > 0\right) } = 2 \dfrac{\dot{s}_{\theta}}{s_{\theta}} 1_{\left( s_{\theta} > 0\right) } \\
    &\dot{s}_{\theta} = \frac{1}{2} \dot{l}_{\theta} s_{\theta} 1_{\left( p_{\theta} > 0\right) }
\end{align*}
$$  
and the Fisher information is defined with 2 different representations as $$ I(\theta) = 4\int \dot{s}_{\theta} \dot{s}^\top_{\theta} d\nu = \int \dot{l}_{\theta} \dot{l}^\top_{\theta} d\mathbb{P}_{\theta} $$, refer to [2, Proposition 2.1.1 ] for more details.  

### Consequences of regular parametrization

The above characterization is almost enough for *asymptotic normality* of maximum likelihood estimators, in that we just need some Lipshitz property of the log likelihood like in [7, Theorem 5.39], this could be achieved with assuming that the true parameter lies in some compact set. A deeper consequence is the notion of *local asymptotic normality*, which says that the log likelihood ratio w.r.t product measures at some reference point $$ \theta $$  
$$
\begin{align*}
    \log \dfrac{d \mathbb{P}^n_{\theta + t/\sqrt{n}}}{d \mathbb{P}^n_{\theta}}
\end{align*}
$$  
could be approximated uniformly over some compact set $$ K \subset \Theta $ and $ \left|t \right| \le M  $$ using a quadratic approximation, upon receiving $$ n $$ samples $$ X_1, \ldots, X_n $$:  
$$
\begin{align*}
    &\log \dfrac{d \mathbb{P}^n_{\theta + t/\sqrt{n}}}{d \mathbb{P}^n_{\theta}} = t^T \mathbb{P}_n \dot{l}_{\theta} - \frac{1}{2} t^T I(\theta) t + R_{n1}(\theta, t) \\
    &\mathbb{P}_n \dot{l}_{\theta + t/\sqrt{n}} - \mathbb{P}_n \dot{l}_{\theta} + I(\theta) t = R_{n2}(\theta, t)
\end{align*}
$$  
where $$ R_{n1}(\theta, t), R_{n2}(\theta, t) $$ converges uniformly over $$ K \subset \Theta $$ and $$ \left|t \right| \le M  $$ in $$ \mathbb{P}_{\theta} $$ probability. And in terms of *convergence of statistical experiments*, a regular parameterization implies upon receiving a large number of samples, the *local model* $$ \left( \mathbb{P}^n_{\theta + t/\sqrt{n}}, t \in \mathbb{R}^d \right) $$ is almost identical to a single observation from a normal model $$ \left( N(t, I^{-1}(\theta)) \right) $$ assuming we known $$ I(\theta) $$, the theory therein is quite involved and refer to [7, Chapter 7, Chapter 9] for a thorough treatment. 

## Estimators

As we have seen from previous section, a good parameterization suffices for a story of maximum likelihood, or at least it works, i.e. we have an asymptotically normal estimator. However it seems to arrive at the "optimal" likelihood principle, we still need to establish some optimality, which amounts to make some careful analysis of some estimators. In general, the *parameter* of interest could be different from what we use to parameterize the model, for example in exponential family distributions, we could always parameterize the model using either the mean or the natural parameter. So it would be more reasonable to consider the parameter of interest as a map $$ v: \mathscr{P} \mapsto \mathbb{R}^q $$, also called *statistical functional*. Then we need to explain the remaining of the story via investigating the asymptotic behavior of $$ \sqrt{n}\left( T_n - v(\mathbb{P})\right) $$ for $$ \mathbb{P} \in \mathscr{P} $$ and all the "legal" estimators $$ T_n $$

### Regularity of estimators

One may feel skeptical about the optimality of likelihood principle as something "too good to be true", this is a somewhat tricky operation here that we *restrict the legal estimators to be the regular ones*, and *regularity* is a very strong restriction since it requires the asymptotic normality in some sense of uniformity. In [2, Section 2.2], regularity is divided into 2 cases:

#### Uniform regularity

requires *uniform weak convergence over compact subsets*, i.e.  
$$
\begin{align*}
    \sup_{\mathbb{P} \in \mathcal{K}}\left| \mathbb{P} g(\sqrt{n}(T_n - v(\mathbb{P}))) - \mathbb{P} g(Z)\right| \rightarrow 0
\end{align*}
$$  
for all bounded continuous function $$ g $$ on $$ \mathbb{R}^q $$, compact subsets $$ \mathcal{K} \subset \mathscr{P} $$, and some limiting variable $$ Z $$, usually we want this to be a zero mean Gaussian variable with covariance matrix $$ \Sigma(\mathbb{P}, T) $$ where $$ T = \{T_n\} $$  

#### Local regularity

 uniform Gaussian regularity appears very strong, and usually we need only uniformity at some specific point. We make use of the previous parameterization $$ \theta \mapsto \mathbb{P}_{\theta} $$ so that $$ T = \{T_n\} $$ is *locally regular* at some point $$ \theta_0 $$ if for any sequence $$ \theta_n $$ such that $$ \sqrt{n}\left| \theta_n - \theta_0\right| $$ stays bounded (This is referred to by Tsiatis in [6] as *local data generating process*), the limit distribution of $ \sqrt{n}(T_n - v(\mathbb{P}_{\theta_n})) $$ is independent of this generating process $$ \theta_n $$. Local regularity is the "notion of regularity" that appears more often like in [7]

### Asymptotic linearity

Before stating the result on why regular estimators ensures the optimality of the likelihood principle, we shall take a little look at a story here: via adopting a representation of parameter as statistical functional, it makes sense to think a little bit about the property of this functional. As we all know we may use the empirical distribution $$ \mathbb{P}_n $$ to approximate the true distribution $$ \mathbb{P} $$ uniformly, so we could somehow expect a local approximation via Fréchet derivative (we assume some other proper metric here i.e. Kolmogorov Smirnov metric):  
$$
\begin{align*}
    v(\mathbb{P}_n) - v(\mathbb{P}) = \dot{v}(\mathbb{P})(\mathbb{P}_n - \mathbb{P}) + o\left(\left\|  \mathbb{P}_n - \mathbb{P}\right\|_{ks}  \right) 
\end{align*}
$$  
By Riesz representation on the space of signed measures, $$ \dot{v}(\mathbb{P})\left( \mathbb{P}_n\right) $$ could be represented using some representor $$ \int \psi (x, \mathbb{P})d\mathbb{P}_n $$, if in addition we have $$ \psi $$ satisfying $$ \int \psi (x, \mathbb{P})d\mathbb{P} = 0 $$, then using the fact that the supreme of empirical process is of the order $$ O_{\mathbb{P}}(1/\sqrt{n}) $$, we've got a pretty expansion here:  
$$
\begin{align*}
    \sqrt{n}\left( v(\mathbb{P}_n) - v(\mathbb{P})\right) = \frac{1}{\sqrt{n}} \sum_{i=1}^{n}\psi(X_i, \mathbb{P}) + o_{\mathbb{P}}(1) \tag{AL}
\end{align*}
$$  
Then it is of no surprise that $$ v(\mathbb{P}_n) $$ is some estimator that is "meaningful", with asymptotic variance $$ \int \psi_{\mathbb{P}}\psi^\top_{\mathbb{P}} d\mathbb{P} $$ (we suppress the dependency of $$ \psi $$ on the sample here and further assume $$ \psi_{\mathbb{P}} \in L_2(\mathbb{P})$$). This kind of phenomenon is called asymptotic linearity for some estimator $$ T = \{T_n\} $$ upon estimating $$ v $$, the corresponding $$ \psi $$, which depends on $$ T $$, is called **influence function** of $$ T = \{T_n\} $$. Moreover, if the residual term is uniform over compact $$ \mathcal{K} $$s, the expansion is called \emph**uniform asymptotic linearity** of $$ T $$. Basically, regularity and asymptotic linearity do not imply each other, so Tsiatis [6] used the term *RAL* to further restrict the estimators of interest. But uniform asymptotic linearity does imply uniform regularity under mild conditions, which could be viewed as a very special case of *Donsker property*, see [2, Proposition 2.2.1]  

### Convolution theorems and information bound

Recall that through all these abstract stuff what we want to do is to argue that  

> If we restrict our consideration of estiamtors to all the regular (in either local or uniform sense) ones, the best one is obtained via MLE  

for this case it's not hard to verify that the influence function of MLE should be the score function, and the corresponding asymptotic variance should be the Fisher information. The only thing remains is to establish the information inequality, which turns out to be a consequence of one of the fundamental master theorems in asymptotic statistics by Hájek and Le Cam.  
Consider estimating a smooth parameter which could be written under regular parameterization as $$ \vartheta(\theta) = v(\mathbb{P}_{\theta}) \in \mathbb{R}^q $$, with elementary facts in mind, define our "best influence function" and "smallest asymptotic variance as":  

#### Efficient influence function for $$ v $$

$$ \tilde{\psi}_{\mathbb{P}_{\theta}}(\cdot | v, \mathscr{P})  = \dot{\vartheta}(\theta) I^{-1}(\theta) \dot{l}_{\theta} $$ as per notation in [2], I'm sort of lazy here and just keep the notation concise and just denote it $$ \tilde{\psi}_{\vartheta(\theta)}(\cdot) $$

#### Information bound for $$ v $$

$$ I^{-1}_{\vartheta}(\theta) =  \dot{\vartheta}(\theta) I^{-1}(\theta) \dot{\vartheta}^\top (\theta) $$  

Then the **convolution theorem** says that *either in the sense of locality or uniformity*, the limiting law of any regular estimator $$ T $$ upon estimating $$ v $$,  is presented by a convolution of a normal variable $$ Z_{\vartheta}(\theta) \sim  N(0, I^{-1}_{\vartheta}(\theta)) $$ and some independent noise $$ \Delta_{\vartheta}(\theta) $$. More generally,  
$$
\begin{align*}\label{conv}
    \begin{pmatrix}
        \sqrt{n}\left( T_n - q(\theta)\right)  - \frac{1}{\sqrt{n}}\sum_{i=1}^{n} \tilde{\psi}_{\vartheta(\theta)}(X_i) \\
        \frac{1}{\sqrt{n}}\sum_{i=1}^{n} \tilde{\psi}_{\vartheta(\theta)}(X_i)
    \end{pmatrix}
    \rightsquigarrow
    \begin{pmatrix}
        \Delta_{\vartheta}(\theta) \\
        Z_{\vartheta}(\theta)
    \end{pmatrix}\tag{CONV}
\end{align*}
$$  
and if $$ q(\cdot) $$ is continuously differentiable, we have the equivalence that the limiting above limiting law is $$ Z_{\vartheta}(\theta) $$ iff $$ T $$ is asymptotically linear with the efficient influence function. Now it's a trivial consequence that *either in the sense of locality or uniformity*  
$$
\begin{align*}
    \Sigma(\mathbb{P}_{\theta}, T) \succeq I^{-1}_{\vartheta}(\theta)
\end{align*}
$$  
hold over regular estimators on the order of nonnegative definite matrices, the equality holds when the noise part of \eqref{conv} $$ \Delta_{\vartheta}(\theta) $$ is degenerate at $$ 0 $$, and this characterizes the likelihood principle for finite dimensional models. The above presentation is taken from [2, Theorem 2.3.1], see also [7, Chapter 8] on some further results like *asymptotic minimax theorems*

### Is regularity that reasonable ?

Up to this point, the only thing that raises skepticism is the restriction on *regularity*, at first glance regularity rules out some estimators that are *unstable*, and this turns out to be kinda stringent, since modern statistics embraces some *irregular estimators* under situations that we adopt some *prior  belief* over the parameter space, the most famous of them are perhaps:  

#### LASSO type estimators [3]

Which essentially are super-efficient estimators over zeros, while under the belief of sparsity, this provides extreme help

#### Empirical bayes methods like James Stein

For estimating multivariate Gaussians, MLE behaves poorly as stated in [4], and shrinkage estimators are the way to go